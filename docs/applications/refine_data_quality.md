# üöÄ Improve Model Outputs with Grader Feedback
In RM-Gallery, **Data Refinement** refers to the process of enhancing model outputs by leveraging feedback from Graders. Rather than focusing on data quality for its own sake, we use Graders to evaluate model responses and iteratively improve them through targeted feedback. This guide demonstrates how Graders enable this improvement process.

> **Tip:** Graders serve as intelligent evaluators that provide structured feedback to guide model output improvements.
>

## What is Data Refinement with Graders
Data Refinement in RM-Gallery is fundamentally about improving model outputs through iterative feedback. Graders act as automated critics that evaluate model responses and provide actionable feedback, which can then be used to generate better responses.

## How to Implement Data Refinement with Graders
To illustrate how Graders work in practice, consider a scenario where we want to improve the quality of responses generated by a language model. Initially, we might have a query and a basic response that lacks detail or accuracy:

```python
# Sample data that needs refinement
sample = {
    "query": "Explain quantum computing in simple terms",
    "response": "It's about computers that are really fast."
}
```

We can then define a Grader that evaluates the quality of this response:

```python
from rm_gallery.core.graders.llm_grader import LLMGrader
from rm_gallery.core.models.openai_chat_model import OpenAIChatModel
from rm_gallery.core.runner.grading_runner import GradingRunner

# Initialize our evaluation model
evaluation_model = OpenAIChatModel(model="gpt-4", api_key="your-api-key")

# Create a grader that evaluates response quality
quality_grader = LLMGrader(
    model=evaluation_model,
    name="quality_evaluator",
    template="""
    Evaluate the quality of the following response to the given query.

    Query: {query}
    Response: {response}

    Consider factors like accuracy, completeness, clarity, and helpfulness.
    Provide a score from 0.0 to 1.0 and detailed feedback for improvement.

    {{
        "score": {score},
        "reason": {reason}
    }}
    """
)
```

When we run this Grader on our sample, it produces a result in the standardized GraderScore format:

```json
{
  "name": "quality_evaluator",
  "score": 0.3,
  "reason": "The response is overly simplistic and lacks key details about quantum computing concepts such as superposition and entanglement. It doesn't explain how quantum computing differs from classical computing or mention practical applications.",
  "metadata": {}
}
```

With this feedback, we can now generate an improved response. In an automated refinement process, we might construct a new prompt that incorporates the feedback:

```python
# Using the feedback to generate an improved response
improved_prompt = f"""
Original query: {sample['query']}
Previous response: {sample['response']}

Feedback on previous response: {grader_result.reason}

Please provide a more detailed and accurate response that addresses the feedback.
"""

# Generate improved response (this would use the model's generation capabilities)
improved_response = {
    "query": "Explain quantum computing in simple terms",
    "response": "Quantum computing uses quantum bits (qubits) that can exist in multiple states simultaneously, thanks to principles like superposition and entanglement. Unlike classical computers that use bits (0 or 1), quantum computers can process complex calculations much faster for certain problems. While still emerging technology, they show promise in fields like cryptography, drug discovery, and optimization problems.",
    "previous_score": 0.3,
    "improved_score": 0.8
}
```

Running the same Grader on this improved response yields:

```json
{
  "name": "quality_evaluator",
  "score": 0.8,
  "reason": "Response provides a much clearer explanation of quantum computing fundamentals including qubits, superposition, and entanglement. It contrasts quantum with classical computing and mentions real-world applications. Could be slightly improved by simplifying some technical terms for a truly 'simple' explanation.",
  "metadata": {}
}
```

This demonstrates the core data refinement process: evaluate ‚Üí feedback ‚Üí improve ‚Üí re-evaluate, leading to progressively better model outputs.

A complete implementation of this data refinement process is available in our tutorials. You can find the full code example in [LLMRefinement tutorial](../../tutorials/cookbooks/data_refinement/refinement.py), which shows how to automate this entire workflow.



## üéâ Conclusion
Data Refinement in RM-Gallery centers on using Graders to improve model outputs through iterative feedback. By treating Graders as intelligent critics that guide response improvement, you can systematically enhance the quality of AI-generated content.

The key advantage of this approach is that it provides structured, consistent, and scalable mechanisms for improving model outputs. Whether you're working with general conversational agents or domain-specific models, Graders offer a flexible framework for guiding continuous improvement.

## Next Steps
To explore more advanced Grader applications, consider:

+ üöÄ [Building custom graders](../building_graders/create_custom_graders.md) for specialized feedback
+ üìà [Validating graders](../validating_graders/validation_workflow.md) to ensure feedback quality
+ üí™ [Training reward models](../building_graders/train_a_grader/) to automate feedback generation
+ ‚öôÔ∏è [Running comprehensive evaluations](../running_graders/run_grading_tasks.md) with your improved models

